{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yl5f30ItZXhe"
   },
   "source": [
    "# What this code does: Top genes selection--> Impute --> Clustering\n",
    "#  Imputation methods included: scVI, gimVI, tangram (You can add more imputation methods in the 'Imputation Evaluator Class easily....)\n",
    "1. Gene Selection Criteria: All genes, top 2000 genes, top 5000 genes\n",
    "2. Leiden Clustering is used when inferencing.\n",
    "3. gimVI and tangram are dedicated for ST data but requires the corresponding scRNA-seq data. In ours case, the respective ST datasets do not have the respective scRNA-seq data. Hence, we use the ST datasets as an alternative of the scRNA-seq datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code can be run in three mode:\n",
    "1) Full Batch Mode: It will look into a directory for '.h5ad' file and run them one by one for all genes, top 2000 genes, top 5000 genes and then save in a csv.\n",
    "2) Semi Batch Mode: It will look into a directory for '.h5ad' file and an external parameter can be set which will decide how many datasets will be processed at once for all genes, top 2000 genes and top 5000 genes.\n",
    "3) Multi-batch mode: It will look into a directory for '.h5ad' file and two external parameters can be set where first parameter will decide how many datasets will be processed at once and second parameter will decide the number of gene selection value from all genes, top 2000 genes and top 5000 genes to be processed for each datasets.\n",
    "\n",
    "Such modes are created for easier usability as different computer configuration can take longer time if all of the run at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjH3MNFD5vWl",
    "outputId": "de6f4a73-6341-4d14-b2a2-1ebe3ffeea2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# @title mount drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsgl7leS6Aic",
    "outputId": "9d0f7631-bcc7-4cd5-b92f-d23f920e81b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scanpy\n",
      "  Downloading scanpy-1.11.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting anndata>=0.8 (from scanpy)\n",
      "  Downloading anndata-0.11.4-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: h5py>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.13.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.4.2)\n",
      "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
      "  Downloading legacy_api_wrap-1.4.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.10.0)\n",
      "Requirement already satisfied: natsort in /usr/local/lib/python3.11/dist-packages (from scanpy) (8.4.0)\n",
      "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.11/dist-packages (from scanpy) (3.4.2)\n",
      "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.60.0)\n",
      "Requirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.0.2)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from scanpy) (24.2)\n",
      "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (2.2.2)\n",
      "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.0.1)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.13)\n",
      "Collecting scikit-learn<1.6.0,>=1.1 (from scanpy)\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.11/dist-packages (from scanpy) (1.15.2)\n",
      "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.13.2)\n",
      "Collecting session-info2 (from scanpy)\n",
      "  Downloading session_info2-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.14.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from scanpy) (4.13.2)\n",
      "Requirement already satisfied: umap-learn!=0.5.0,>=0.5 in /usr/local/lib/python3.11/dist-packages (from scanpy) (0.5.7)\n",
      "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
      "  Downloading array_api_compat-1.11.2-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7->scanpy) (2.9.0.post0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.57->scanpy) (0.43.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->scanpy) (2025.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.6.0,>=1.1->scanpy) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7->scanpy) (1.17.0)\n",
      "Downloading scanpy-1.11.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anndata-0.11.4-py3-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading legacy_api_wrap-1.4.1-py3-none-any.whl (10.0 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading session_info2-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading array_api_compat-1.11.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: session-info2, legacy-api-wrap, array-api-compat, scikit-learn, anndata, scanpy\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.6.1\n",
      "    Uninstalling scikit-learn-1.6.1:\n",
      "      Successfully uninstalled scikit-learn-1.6.1\n",
      "Successfully installed anndata-0.11.4 array-api-compat-1.11.2 legacy-api-wrap-1.4.1 scanpy-1.11.1 scikit-learn-1.5.2 session-info2-0.1.2\n",
      "Collecting magic-impute\n",
      "  Downloading magic_impute-3.0.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from magic-impute) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from magic-impute) (1.15.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from magic-impute) (3.10.0)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from magic-impute) (1.5.2)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from magic-impute) (1.0.0)\n",
      "Collecting tasklogger>=1.0.0 (from magic-impute)\n",
      "  Downloading tasklogger-1.2.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting graphtools>=1.4.0 (from magic-impute)\n",
      "  Downloading graphtools-1.5.3-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.11/dist-packages (from magic-impute) (2.2.2)\n",
      "Collecting scprep>=1.0 (from magic-impute)\n",
      "  Downloading scprep-1.2.3-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting pygsp>=0.5.1 (from graphtools>=1.4.0->magic-impute)\n",
      "  Downloading PyGSP-0.5.1-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: Deprecated in /usr/local/lib/python3.11/dist-packages (from graphtools>=1.4.0->magic-impute) (1.2.18)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->magic-impute) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->magic-impute) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25->magic-impute) (2025.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->magic-impute) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->magic-impute) (3.6.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from scprep>=1.0->magic-impute) (4.4.2)\n",
      "Collecting pandas>=0.25 (from magic-impute)\n",
      "  Downloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from scprep>=1.0->magic-impute) (24.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->magic-impute) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->magic-impute) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->magic-impute) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->magic-impute) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->magic-impute) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->magic-impute) (3.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25->magic-impute) (1.17.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->graphtools>=1.4.0->magic-impute) (1.17.2)\n",
      "Downloading magic_impute-3.0.0-py3-none-any.whl (15 kB)\n",
      "Downloading graphtools-1.5.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading scprep-1.2.3-py3-none-any.whl (94 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.1/94.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.0.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tasklogger-1.2.0-py3-none-any.whl (14 kB)\n",
      "Downloading PyGSP-0.5.1-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tasklogger, pygsp, pandas, scprep, graphtools, magic-impute\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.2\n",
      "    Uninstalling pandas-2.2.2:\n",
      "      Successfully uninstalled pandas-2.2.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
      "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
      "mizani 0.13.3 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
      "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed graphtools-1.5.3 magic-impute-3.0.0 pandas-2.0.3 pygsp-0.5.1 scprep-1.2.3 tasklogger-1.2.0\n",
      "Collecting igraph\n",
      "  Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting texttable>=1.6.2 (from igraph)\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: texttable, igraph\n",
      "Successfully installed igraph-0.11.8 texttable-1.7.0\n",
      "Collecting leidenalg\n",
      "  Downloading leidenalg-0.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: igraph<0.12,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from leidenalg) (0.11.8)\n",
      "Requirement already satisfied: texttable>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from igraph<0.12,>=0.10.0->leidenalg) (1.7.0)\n",
      "Downloading leidenalg-0.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: leidenalg\n",
      "Successfully installed leidenalg-0.10.2\n",
      "Collecting autoimpute\n",
      "  Downloading autoimpute-0.14.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from autoimpute) (2.0.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from autoimpute) (1.15.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from autoimpute) (2.0.3)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.11/dist-packages (from autoimpute) (0.14.4)\n",
      "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (from autoimpute) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from autoimpute) (1.5.2)\n",
      "Requirement already satisfied: pymc in /usr/local/lib/python3.11/dist-packages (from autoimpute) (5.22.0)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from autoimpute) (0.13.2)\n",
      "Requirement already satisfied: missingno in /usr/local/lib/python3.11/dist-packages (from autoimpute) (0.5.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from missingno->autoimpute) (3.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->autoimpute) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->autoimpute) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas->autoimpute) (2025.2)\n",
      "Requirement already satisfied: arviz>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from pymc->autoimpute) (0.21.0)\n",
      "Requirement already satisfied: cachetools>=4.2.1 in /usr/local/lib/python3.11/dist-packages (from pymc->autoimpute) (5.5.2)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from pymc->autoimpute) (3.1.1)\n",
      "Requirement already satisfied: pytensor<2.31,>=2.30.2 in /usr/local/lib/python3.11/dist-packages (from pymc->autoimpute) (2.30.3)\n",
      "Requirement already satisfied: rich>=13.7.1 in /usr/local/lib/python3.11/dist-packages (from pymc->autoimpute) (13.9.4)\n",
      "Requirement already satisfied: threadpoolctl<4.0.0,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from pymc->autoimpute) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pymc->autoimpute) (4.13.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->autoimpute) (1.4.2)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels->autoimpute) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels->autoimpute) (24.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost->autoimpute) (2.21.5)\n",
      "Requirement already satisfied: setuptools>=60.0.0 in /usr/local/lib/python3.11/dist-packages (from arviz>=0.13.0->pymc->autoimpute) (75.2.0)\n",
      "Requirement already satisfied: xarray>=2022.6.0 in /usr/local/lib/python3.11/dist-packages (from arviz>=0.13.0->pymc->autoimpute) (2025.3.1)\n",
      "Requirement already satisfied: h5netcdf>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from arviz>=0.13.0->pymc->autoimpute) (1.6.1)\n",
      "Requirement already satisfied: xarray-einstats>=0.3 in /usr/local/lib/python3.11/dist-packages (from arviz>=0.13.0->pymc->autoimpute) (0.8.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->missingno->autoimpute) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->missingno->autoimpute) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->missingno->autoimpute) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->missingno->autoimpute) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->missingno->autoimpute) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->missingno->autoimpute) (3.2.3)\n",
      "Requirement already satisfied: filelock>=3.15 in /usr/local/lib/python3.11/dist-packages (from pytensor<2.31,>=2.30.2->pymc->autoimpute) (3.18.0)\n",
      "Requirement already satisfied: etuples in /usr/local/lib/python3.11/dist-packages (from pytensor<2.31,>=2.30.2->pymc->autoimpute) (0.3.9)\n",
      "Requirement already satisfied: logical-unification in /usr/local/lib/python3.11/dist-packages (from pytensor<2.31,>=2.30.2->pymc->autoimpute) (0.4.6)\n",
      "Requirement already satisfied: miniKanren in /usr/local/lib/python3.11/dist-packages (from pytensor<2.31,>=2.30.2->pymc->autoimpute) (1.0.3)\n",
      "Requirement already satisfied: cons in /usr/local/lib/python3.11/dist-packages (from pytensor<2.31,>=2.30.2->pymc->autoimpute) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->autoimpute) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->pymc->autoimpute) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.7.1->pymc->autoimpute) (2.19.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from h5netcdf>=1.0.2->arviz>=0.13.0->pymc->autoimpute) (3.13.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.7.1->pymc->autoimpute) (0.1.2)\n",
      "Collecting pandas (from autoimpute)\n",
      "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from logical-unification->pytensor<2.31,>=2.30.2->pymc->autoimpute) (0.12.1)\n",
      "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.11/dist-packages (from logical-unification->pytensor<2.31,>=2.30.2->pymc->autoimpute) (1.0.0)\n",
      "Downloading autoimpute-0.14.1-py3-none-any.whl (98 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.1/98.1 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pandas, autoimpute\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.0.3\n",
      "    Uninstalling pandas-2.0.3:\n",
      "      Successfully uninstalled pandas-2.0.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "scprep 1.2.3 requires pandas<2.1,>=0.25, but you have pandas 2.2.3 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed autoimpute-0.14.1 pandas-2.2.3\n"
     ]
    }
   ],
   "source": [
    "# @title installing packages\n",
    "!pip install scanpy\n",
    "!pip install magic-impute\n",
    "!pip install igraph\n",
    "!pip3 install leidenalg\n",
    "!pip install fancyimpute\n",
    "!pip install -U scvi-tools\n",
    "!pip install tangram-sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y6CnuxnE45t5"
   },
   "outputs": [],
   "source": [
    "# @title Importing packages\n",
    "\n",
    "# System & utility\n",
    "import os\n",
    "import time\n",
    "import psutil\n",
    "import tracemalloc\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Core scientific packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning and preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    normalized_mutual_info_score,\n",
    "    adjusted_rand_score,\n",
    "    adjusted_mutual_info_score,\n",
    "    homogeneity_score\n",
    ")\n",
    "\n",
    "# Matrix and sparse operations\n",
    "from scipy import sparse\n",
    "from scipy.sparse import issparse, csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Imputation methods\n",
    "import magic\n",
    "from fancyimpute import SoftImpute\n",
    "\n",
    "# Single-cell packages\n",
    "import scanpy as sc\n",
    "from scvi.external import GIMVI\n",
    "import tangram as tg\n",
    "import scvi\n",
    "\n",
    "# Uncomment these if needed later\n",
    "# import scarches as sca\n",
    "# import dca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CZcT-tbwgtRK"
   },
   "outputs": [],
   "source": [
    "# @title Imputation Evaluator\n",
    "\n",
    "class ImputationEvaluator:\n",
    "    def __init__(self, dataset_path, n_top_genes=2000):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.dataset_name = os.path.basename(dataset_path)\n",
    "\n",
    "        if n_top_genes == 'all':\n",
    "            self.n_top_genes = None\n",
    "        else:\n",
    "            self.n_top_genes = int(n_top_genes)\n",
    "\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load dataset and select top highly variable genes.\"\"\"\n",
    "        print(f\"\\nLoading dataset: {self.dataset_path}\")\n",
    "        self.adata = sc.read_h5ad(self.dataset_path)\n",
    "\n",
    "        # Convert sparse matrix to dense if necessary\n",
    "        # self.adata = self.adata.copy()\n",
    "        X = self.adata.X\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "            self.adata.X = X\n",
    "\n",
    "\n",
    "        print(f\"In the Imputation Evaluator the n_top_genes: {self.n_top_genes}\\n\\n\")\n",
    "        print(f\"Original adata shape: {self.adata.shape}\")\n",
    "\n",
    "        # Determine ground truth label key\n",
    "        if 'annotation' in self.adata.obs.columns:\n",
    "            self.annotation_key = 'annotation'\n",
    "        elif 'CellType' in self.adata.obs.columns:\n",
    "            self.annotation_key = 'CellType'\n",
    "        else:\n",
    "            raise ValueError(f\"No 'annotation' or 'CellType' found in obs columns for {self.dataset_name}\")\n",
    "\n",
    "        self.size_after_preprocessing = self.adata.shape\n",
    "\n",
    "        if self.n_top_genes is not None:\n",
    "          print(f\"top genes the class got: {self.n_top_genes}\\n\")\n",
    "          print(f\"type of this var: {type(self.n_top_genes)}\\n\")\n",
    "          # Keep the top highly variable genes\n",
    "          sc.pp.highly_variable_genes(self.adata, flavor=\"seurat\", n_top_genes=self.n_top_genes)\n",
    "          self.adata = self.adata[:, self.adata.var['highly_variable']]\n",
    "          self.size_after_top_genes = self.adata.shape\n",
    "          print(f\"Adata shape after selecting top {self.n_top_genes} genes: {self.adata.shape}\")\n",
    "        else:\n",
    "          self.size_after_top_genes = self.adata.shape\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_sparsity(X):\n",
    "        \"\"\"Calculate sparsity of a matrix.\"\"\"\n",
    "        if issparse(X):\n",
    "            X = X.toarray()\n",
    "        zero_elements = np.sum(X == 0)\n",
    "        total_elements = X.size\n",
    "        return 100.0 * float(zero_elements) / float(total_elements)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def scVI_impute(adata_imputed, max_epochs=350, latent_dim=32, use_gpu=True):\n",
    "        \"\"\"\n",
    "        Train scVI and return imputed normalized expression (adata.X).\n",
    "\n",
    "        Parameters:\n",
    "            adata_imputed : AnnData\n",
    "                AnnData object containing raw counts or approximated counts.\n",
    "            max_epochs : int\n",
    "                Maximum number of training epochs.\n",
    "            latent_dim : int or None\n",
    "                Dimension of latent space for the VAE.\n",
    "            use_gpu : bool\n",
    "                Whether to use GPU (if available).\n",
    "\n",
    "        Returns:\n",
    "            imputed_expression : np.ndarray\n",
    "                Dense matrix of normalized expression (imputed values).\n",
    "        \"\"\"\n",
    "        n_cores = os.cpu_count()\n",
    "        torch.set_num_threads(n_cores - 1)  # Use 11 threads if 12 cores detected\n",
    "        adata_scVI = adata_imputed.copy()\n",
    "        SCVI = scvi.model.SCVI\n",
    "        # Setup for scVI\n",
    "        print(\"Setting up scVI...\")\n",
    "        SCVI.setup_anndata(adata_scVI)\n",
    "\n",
    "        # Create the scVI model\n",
    "        print(\"Creating scVI model...\")\n",
    "        if latent_dim is not None:\n",
    "            vae = SCVI(adata_scVI, n_latent=latent_dim)\n",
    "        else:\n",
    "            vae = SCVI(adata_scVI)\n",
    "\n",
    "        # Train model on GPU if available\n",
    "        device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Training on {device}...\")\n",
    "        vae.train(max_epochs=max_epochs, accelerator=device, devices=1)\n",
    "\n",
    "        # Get imputed/normalized values and assign to X\n",
    "        print(\"Fetching normalized expression...\")\n",
    "        imputed_values = vae.get_normalized_expression()\n",
    "        print(\"Finished fetching.\")\n",
    "\n",
    "        # Assign to .X and return\n",
    "        adata_scVI.X = imputed_values\n",
    "        return adata_scVI.X\n",
    "\n",
    "    @staticmethod\n",
    "\n",
    "    def gimVI_impute(adata_imputed, max_epochs=400, n_latent=40, use_gpu=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Train gimVI imputation and return imputed normalized expresssion (adata.X) \n",
    "        \"\"\"\n",
    "        adata_seq = adata_imputed.copy()\n",
    "        spatial_adata = adata_imputed.copy()\n",
    "\n",
    "        # Setup for gimVI\n",
    "        print(\"Setting up gimVI...\")\n",
    "        GIMVI.setup_anndata(adata_seq)\n",
    "        GIMVI.setup_anndata(spatial_adata)\n",
    "\n",
    "        # Create the gimVI model\n",
    "        print(\"Creating gimVI model...\")\n",
    "        gimVi_spatial = spatial_adata\n",
    "        gimVi_expression = adata_seq\n",
    "\n",
    "        model = GIMVI(gimVi_expression, gimVi_spatial, n_latent=n_latent)\n",
    "\n",
    "        # Train model on GPU if available\n",
    "        accelerator = \"gpu\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Training on {accelerator}.....\")\n",
    "        model.train(max_epochs=max_epochs, accelerator=accelerator, devices=1)\n",
    "        \n",
    "        # Get imputed/normalised values\n",
    "        print(\"Fetching normalized expression....\")\n",
    "        _, imputed_values = model.get_imputed_values()\n",
    "        imputed_values = csr_matrix(imputed_values)\n",
    "        print(\"Finished fetching!\")\n",
    "        \n",
    "        return imputed_values\n",
    "\n",
    "    @staticmethod\n",
    "    def tangram_impute(adata_imputed, max_epochs=200, use_gpu=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Train tangram imputation and return imputed normalized expresssion (adata.X) \n",
    "        \"\"\"\n",
    "        \n",
    "        adata_seq = adata_imputed.copy()\n",
    "        spatial_adata = adata_imputed.copy()\n",
    "\n",
    "        Xdense = adata_seq.X\n",
    "        if issparse(Xdense):\n",
    "            adata_seq.X = Xdense.toarray()\n",
    "\n",
    "        Xdense = spatial_adata.X\n",
    "        if issparse(Xdense):\n",
    "            spatial_adata.X = Xdense.toarray()\n",
    "\n",
    "        markers = list(set.intersection(set(adata_seq.var_names), set(spatial_adata.var_names))) # get common genes/they are all common\n",
    "\n",
    "        # Setup for tangram\n",
    "        print(\"Setting up tangram...\")\n",
    "        tg.pp_adatas(adata_seq, spatial_adata, genes=markers)\n",
    "    \n",
    "       \n",
    "        # Train model on GPU if available\n",
    "        device = \"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Training on {device}.....\")\n",
    "        ad_map = tg.map_cells_to_space(\n",
    "            adata_seq,\n",
    "            spatial_adata,\n",
    "            mode=\"cells\",\n",
    "            density_prior=\"rna_count_based\",\n",
    "            num_epochs=max_epochs,\n",
    "            device=device,  # or: cpu\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Get imputed/normalised values\n",
    "        print(\"Projecting gene expression...\")\n",
    "        ad_ge = tg.project_genes(adata_map=ad_map, adata_sc=adata_seq)\n",
    "        print(\"Done.\")\n",
    "\n",
    "        imputed_values = csr_matrix(ad_ge.X)\n",
    "\n",
    "        return imputed_values\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"............More imputation\n",
    "    methods to add....................\"\"\"\n",
    "    # def dummy_imputation_X(X):\n",
    "    #     \"\"\"Dummy imputation method X (for placeholder).\"\"\"\n",
    "    #     return X + np.random.normal(0, 0.01, size=X.shape)\n",
    "\n",
    "\n",
    "    # def dummy_imputation_Y(X):\n",
    "    #     \"\"\"Dummy imputation method Y (for placeholder).\"\"\"\n",
    "    #     return np.clip(X * 1.01, 0, None)\n",
    "\n",
    "    def perform_clustering(self, adata, cluster_key):\n",
    "        \"\"\"Perform PCA, neighbors, UMAP and Leiden clustering.\"\"\"\n",
    "        sc.pp.pca(adata)\n",
    "        sc.pp.neighbors(adata)\n",
    "        sc.tl.umap(adata)\n",
    "        sc.tl.leiden(adata, key_added=cluster_key, directed=False, n_iterations=2)\n",
    "\n",
    "    def perform_clustering_with_plot(self, adata, cluster_key, dataset_name=None, n_top_genes=2000):\n",
    "        \"\"\"Perform PCA, neighbors, UMAP, Leiden clustering, and plot UMAP.\"\"\"\n",
    "\n",
    "        # Step 1: Dimensionality reduction and clustering\n",
    "        sc.pp.pca(adata)\n",
    "        sc.pp.neighbors(adata)\n",
    "        sc.tl.umap(adata)\n",
    "        sc.tl.leiden(adata, key_added=cluster_key, directed=False, n_iterations=2)\n",
    "\n",
    "        # Step 2: Title for UMAP plot\n",
    "        if n_top_genes is not None:\n",
    "            title = f\"UMAP - {dataset_name} | Top {n_top_genes} genes\" if dataset_name and n_top_genes else \"UMAP\"\n",
    "        else:\n",
    "            title = f\"UMAP - {dataset_name} | Full Data\" if dataset_name else \"UMAP\"\n",
    "\n",
    "        adata.obs[cluster_key] = adata.obs[cluster_key].astype(\"category\")\n",
    "        # Step 3: Show UMAP plot\n",
    "        sc.pl.umap(\n",
    "            adata,\n",
    "            color=cluster_key,\n",
    "            title=title,\n",
    "            legend_loc='on data',\n",
    "            frameon=False,\n",
    "            show=True\n",
    "        )\n",
    "\n",
    "        # Commented out for visium...\n",
    "        # # Step 4: Title for predicted label on the sptial plot\n",
    "        # if n_top_genes is not None:\n",
    "        #     title = f\"Predicted Spatial Plot - {dataset_name} | Top {n_top_genes} genes\" if dataset_name and n_top_genes else \"Predicted Spatial Plot\"\n",
    "        # else:\n",
    "        #     title = f\"Predicted Spatial Plot - {dataset_name} | Full Data\" if dataset_name else \"Predicted Spatial Plot\"\n",
    "        # # Step 5: Show predicted spatial plot\n",
    "\n",
    "        # sc.pl.spatial(\n",
    "        #     adata,\n",
    "        #     color=cluster_key,\n",
    "        #     title=title,\n",
    "        #     spot_size=2\n",
    "        # )\n",
    "\n",
    "    def plot_spatial_with_predicted_labels(self, adata, annotation_key='annotation', dataset_name=None, s=4, n_top_genes=2000):\n",
    "        \"\"\"\n",
    "        Plots 2D spatial coordinates of cells colored by true labels.\n",
    "\n",
    "        Parameters:\n",
    "        - adata: AnnData object with adata.obsm['spatial'] and adata.obs[annotation_key]\n",
    "        - annotation_key: Column in adata.obs for true labels (e.g., 'annotation', 'cell_type')\n",
    "        - dataset_name: Optional dataset name for the plot title\n",
    "        - s: Marker size\n",
    "        \"\"\"\n",
    "\n",
    "        # Step 1: Get spatial coordinates\n",
    "        spatial_coords = adata.obsm['spatial']\n",
    "        x = spatial_coords[:, 0]\n",
    "        y = spatial_coords[:, 1]\n",
    "\n",
    "        # Step 2: Prepare labels\n",
    "        labels = adata.obs[self.annotation_key]\n",
    "        if labels.dtype.name == 'category' or labels.dtype == object:\n",
    "            le = LabelEncoder()\n",
    "            color_labels = le.fit_transform(labels)\n",
    "            label_names = le.classes_\n",
    "        else:\n",
    "            color_labels = labels\n",
    "            label_names = np.unique(labels)\n",
    "\n",
    "        # Step 3: Plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        scatter = plt.scatter(x, y, c=color_labels, cmap='tab20', s=s, alpha=0.8)\n",
    "        plt.gca().invert_yaxis()  # Optional: if spatial coordinates are top-down\n",
    "        plt.title(f\"Spatial Plot (Predicted Labels) - {dataset_name}|{n_top_genes} top genes\" if dataset_name else \"Spatial Plot (Predicted Labels)\")\n",
    "        plt.xlabel(\"Spatial 1\")\n",
    "        plt.ylabel(\"Spatial 2\")\n",
    "\n",
    "        # Create legend\n",
    "        handles = [plt.Line2D([0], [0], marker='o', color='w', label=label,\n",
    "                            markerfacecolor=scatter.cmap(scatter.norm(i)), markersize=8)\n",
    "                for i, label in enumerate(label_names)]\n",
    "        plt.legend(handles=handles, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_spatial_with_true_labels(self, adata, annotation_key='annotation', dataset_name=None, spot_size=2):\n",
    "        \"\"\"\n",
    "        Plots spatial coordinates of cells using scanpy's built-in spatial plot,\n",
    "        colored by true labels (e.g., cell types).\n",
    "\n",
    "        Parameters:\n",
    "        - adata: AnnData object with adata.obsm['spatial'] and adata.obs[annotation_key]\n",
    "        - annotation_key: Column in adata.obs for true labels (e.g., 'annotation', 'cell_type')\n",
    "        - dataset_name: Optional dataset name for title\n",
    "        - spot_size: Marker size for plotting\n",
    "        \"\"\"\n",
    "        if annotation_key not in adata.obs:\n",
    "            raise ValueError(f\"'{annotation_key}' not found in adata.obs\")\n",
    "\n",
    "        \n",
    "        sc.pl.spatial(\n",
    "            adata,\n",
    "            color=annotation_key,\n",
    "            title=f\"Spatial Plot - {dataset_name}\" if dataset_name else \"Spatial Plot (True Labels)\",\n",
    "            spot_size=spot_size\n",
    "        )\n",
    "\n",
    "\n",
    "    def evaluate_clustering(self, adata, cluster_key):\n",
    "        \"\"\"Compute ARI, NMI, AMI, and Homogeneity scores.\"\"\"\n",
    "        true_labels = adata.obs[self.annotation_key]\n",
    "        predicted_labels = adata.obs[cluster_key]\n",
    "\n",
    "        ari = adjusted_rand_score(true_labels, predicted_labels)\n",
    "        nmi = normalized_mutual_info_score(true_labels, predicted_labels)\n",
    "        ami = adjusted_mutual_info_score(true_labels, predicted_labels)\n",
    "        homo = homogeneity_score(true_labels, predicted_labels)\n",
    "\n",
    "        return ari, nmi, ami, homo\n",
    "\n",
    "    def run_full_evaluation(self):\n",
    "        \"\"\"\n",
    "        Run baseline clustering and multiple imputations.\n",
    "        Returns: dict with all results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "\n",
    "        # Define available imputations\n",
    "        imputation_methods = {\n",
    "            'scVI': self.scVI_impute,\n",
    "            'gimVI': self.gimVI_impute,\n",
    "            'tangram': self.tangram_impute\n",
    "        }\n",
    "        # imputation_methods = {\n",
    "        #     'magic': self.magic_impute,\n",
    "        #     'alra': self.alra_impute,\n",
    "        #     'dummy_X': self.dummy_imputation_X,\n",
    "        #     'dummy_Y': self.dummy_imputation_Y,\n",
    "        # }\n",
    "\n",
    "        # Calculate initial sparsity\n",
    "    \n",
    "        results['Raw zero Exp val (%)'] = self.calculate_sparsity(self.adata.X)\n",
    "\n",
    "        # True Labels plot\n",
    "        print(f\"True Labels plot for the dataset {self.dataset_name}..\\n\")\n",
    "        self.plot_spatial_with_true_labels(self.adata, dataset_name=self.dataset_name)\n",
    "        # Baseline clustering\n",
    "        print(\"performing baseline clustering..\\n\")\n",
    "        ari_scores, nmi_scores, ami_scores, homo_scores = [], [], [], []\n",
    "\n",
    "        for i in range(5):\n",
    "            print(f\"Base Clustering Run {i+1}...\")\n",
    "            # self.perform_clustering(self.adata, cluster_key=\"clusters_original\")\n",
    "            self.perform_clustering_with_plot(self.adata, cluster_key=\"clusters_original\", dataset_name=self.dataset_name, n_top_genes=self.n_top_genes)\n",
    "            ari, nmi, ami, homo = self.evaluate_clustering(self.adata, \"clusters_original\")\n",
    "\n",
    "            ari_scores.append(ari)\n",
    "            nmi_scores.append(nmi)\n",
    "            ami_scores.append(ami)\n",
    "            homo_scores.append(homo)\n",
    "\n",
    "        # Compute mean values across the 5 runs\n",
    "        base_ari = np.mean(ari_scores)\n",
    "        base_nmi = np.mean(nmi_scores)\n",
    "        base_ami = np.mean(ami_scores)\n",
    "        base_homo = np.mean(homo_scores)\n",
    "\n",
    "        results.update({\n",
    "            'Base ARI': base_ari,\n",
    "            'Base NMI': base_nmi,\n",
    "            'Base AMI': base_ami,\n",
    "            'Base HOMO': base_homo,\n",
    "        })\n",
    "\n",
    "        # Perform imputations\n",
    "        for method_name, imputation_function in tqdm(imputation_methods.items(), desc=\"Running Imputations\", leave=False):\n",
    "            adata_imputed = self.adata.copy()\n",
    "\n",
    "            X = adata_imputed.X\n",
    "            if issparse(X):\n",
    "                X = X.toarray().astype(np.float16)\n",
    "                adata_imputed.X = X\n",
    "            else:\n",
    "                X = X.astype(np.float16)\n",
    "                adata_imputed.X = X\n",
    "\n",
    "            tracemalloc.start()\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Apply imputation\n",
    "            adata_imputed.X = imputation_function(adata_imputed)\n",
    "\n",
    "            end_time = time.time()\n",
    "            current, peak = tracemalloc.get_traced_memory()\n",
    "            runtime = end_time - start_time\n",
    "            memory = peak / (1024 ** 2)  # MB\n",
    "            tracemalloc.stop()\n",
    "\n",
    "            # Clustering after imputation\n",
    "            cluster_key = f\"clusters_{method_name}\"\n",
    "            print(f\"performing clustering on the {method_name} imputation..\\n\")\n",
    "\n",
    "            ari_scores, nmi_scores, ami_scores, homo_scores = [], [], [], []\n",
    "\n",
    "            for i in range(5):\n",
    "              print(f\"Clustering Run {i+1}...\")\n",
    "              # self.perform_clustering(adata_imputed, cluster_key=cluster_key)\n",
    "              self.perform_clustering_with_plot(adata_imputed, cluster_key=cluster_key, dataset_name=self.dataset_name, n_top_genes=self.n_top_genes)\n",
    "              self.plot_spatial_with_predicted_labels(adata_imputed, annotation_key=cluster_key, dataset_name=self.dataset_name, n_top_genes=self.n_top_genes)\n",
    "              ari, nmi, ami, homo = self.evaluate_clustering(adata_imputed, cluster_key)\n",
    "\n",
    "              ari_scores.append(ari)\n",
    "              nmi_scores.append(nmi)\n",
    "              ami_scores.append(ami)\n",
    "              homo_scores.append(homo)\n",
    "\n",
    "            # Compute mean values across the 5 runs\n",
    "            ari = np.mean(ari_scores)\n",
    "            nmi = np.mean(nmi_scores)\n",
    "            ami = np.mean(ami_scores)\n",
    "            homo = np.mean(homo_scores)\n",
    "\n",
    "            # Save results\n",
    "            results.update({\n",
    "                f'ARI_{method_name}': ari,\n",
    "                f'NMI_{method_name}': nmi,\n",
    "                f'AMI_{method_name}': ami,\n",
    "                f'HOMO_{method_name}': homo,\n",
    "                f'{method_name} zero Exp val (%)': self.calculate_sparsity(adata_imputed.X),\n",
    "                f'{method_name} Runtime (s)': runtime,\n",
    "                f'{method_name} Memory (MB)': memory,\n",
    "            })\n",
    "\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "1n53yIU8e5by"
   },
   "outputs": [],
   "source": [
    "# @title run_on_multiple_datasets\n",
    "\n",
    "def run_on_multiple_datasets(folder_path, n_top_genes=2000):\n",
    "    \"\"\"Run evaluation on all datasets and save results to CSV.\"\"\"\n",
    "    datasets = [f for f in os.listdir(folder_path) if f.endswith('.h5ad')]\n",
    "    all_results = []\n",
    "\n",
    "    for dataset_name in tqdm(datasets, desc=\"Datasets\"):\n",
    "        dataset_path = os.path.join(folder_path, dataset_name)\n",
    "        evaluator = ImputationEvaluator(dataset_path, n_top_genes)\n",
    "\n",
    "        results = evaluator.run_full_evaluation()\n",
    "        result_row = {\n",
    "            'Dataset Name': evaluator.dataset_name,\n",
    "            'Size After pre-processing': f\"{evaluator.size_after_preprocessing[0]}x{evaluator.size_after_preprocessing[1]}\",\n",
    "            'Size After selecting top genes': f\"{evaluator.size_after_top_genes[0]}x{evaluator.size_after_top_genes[1]}\",\n",
    "            'Base ARI': results['Base ARI'],\n",
    "            'Base NMI': results['Base NMI'],\n",
    "            'Base AMI': results['Base AMI'],\n",
    "            'Base HOMO': results['Base HOMO'],\n",
    "            'Raw zero Exp val (%)': results['Raw zero Exp val (%)'],\n",
    "            'Cluster Algo': 'Leiden',\n",
    "            'top_genes': n_top_genes\n",
    "        }\n",
    "\n",
    "        # Add all imputation-specific results dynamically\n",
    "        for key, value in results.items():\n",
    "            if key not in result_row:\n",
    "                result_row[key] = value\n",
    "\n",
    "        all_results.append(result_row)\n",
    "\n",
    "    # convert the results to a dataframe\n",
    "    df_results = pd.DataFrame(all_results)\n",
    "\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6OMpYW7PicXJ"
   },
   "outputs": [],
   "source": [
    "# @title run_on_single_dataset\n",
    "\n",
    "def run_on_single_dataset(dataset_path, n_top_genes):\n",
    "    \"\"\"Process a single dataset, evaluate, and append results to CSV.\"\"\"\n",
    "\n",
    "    evaluator = ImputationEvaluator(dataset_path, n_top_genes)\n",
    "    results = evaluator.run_full_evaluation()\n",
    "\n",
    "    result_row = {\n",
    "        'Dataset Name': evaluator.dataset_name,\n",
    "        'Size After pre-processing': f\"{evaluator.size_after_preprocessing[0]}x{evaluator.size_after_preprocessing[1]}\",\n",
    "        'Size After selecting top genes': f\"{evaluator.size_after_top_genes[0]}x{evaluator.size_after_top_genes[1]}\",\n",
    "        'Base ARI': results['Base ARI'],\n",
    "        'Base NMI': results['Base NMI'],\n",
    "        'Base AMI': results['Base AMI'],\n",
    "        'Base HOMO': results['Base HOMO'],\n",
    "        'Raw zero Exp val (%)': results['Raw zero Exp val (%)'],\n",
    "        'Cluster Algo': 'Leiden',\n",
    "        'top_genes': n_top_genes\n",
    "    }\n",
    "\n",
    "    # Add dynamic imputation-specific results\n",
    "    for key, value in results.items():\n",
    "        if key not in result_row:\n",
    "            result_row[key] = value\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_row = pd.DataFrame([result_row])\n",
    "\n",
    "    return df_row\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SRAmU9F2kK8f"
   },
   "outputs": [],
   "source": [
    "# @title post processing after evaluating imputations\n",
    "\n",
    "def reorder_columns(df):\n",
    "  # Define the base columns\n",
    "  base_keys = ['Dataset Name', 'Size After pre-processing', 'Size After selecting top genes', 'Cluster Algo', 'top_genes']\n",
    "\n",
    "  # Dynamically collect other metric-type columns\n",
    "  ari_keys = [col for col in df.columns if 'ARI' in col]\n",
    "  nmi_keys = [col for col in df.columns if 'NMI' in col]\n",
    "  ami_keys = [col for col in df.columns if 'AMI' in col]\n",
    "  homo_keys = [col for col in df.columns if 'HOMO' in col]\n",
    "  zero_keys = [col for col in df.columns if 'zero' in col]\n",
    "  runtime_keys = [col for col in df.columns if 'Runtime' in col]\n",
    "  memory_keys = [col for col in df.columns if 'Memory' in col]\n",
    "\n",
    "  # Catch any columns not included above\n",
    "  all_collected = set(base_keys + ari_keys + nmi_keys + ami_keys + homo_keys + zero_keys + runtime_keys + memory_keys)\n",
    "  remaining_keys = [col for col in df.columns if col not in all_collected]\n",
    "\n",
    "  # Reorder the DataFrame\n",
    "  ordered_cols = base_keys + ari_keys + nmi_keys + ami_keys + homo_keys + zero_keys + runtime_keys + memory_keys + remaining_keys\n",
    "  df = df[ordered_cols]\n",
    "\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xpNCF2IDLRJo",
    "outputId": "b4239a5f-209d-4662-d0a0-cc1b9578f7c4"
   },
   "outputs": [],
   "source": [
    "# @title main function to run in Full Batch Mode\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  dataset_path = 'D:/VM Data/thesis/Analysis/visium/'\n",
    "  n_top = ['all', 2000, 5000]\n",
    "  # List to collect DataFrames\n",
    "  all_results = []\n",
    "  output_csv = f\"visium_magic_knn_soft_simple_imputation_results.csv\"\n",
    "\n",
    "  for n in n_top:\n",
    "    # Run the function\n",
    "    df_results = run_on_multiple_datasets(dataset_path, n_top_genes = n)\n",
    "    # Collect in list\n",
    "    all_results.append(df_results)\n",
    "\n",
    "  # Merge all DataFrames\n",
    "  final_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "  # Sort by Dataset name, then by Top_Genes value\n",
    "  final_df = final_df.sort_values(by=[\"Dataset Name\", \"top_genes\"]).reset_index(drop=True)\n",
    "\n",
    "  # reorder the results\n",
    "  df_results_reordered = reorder_columns(final_df)\n",
    "\n",
    "  # save to csv\n",
    "  output_path = os.path.join(dataset_path, output_csv)\n",
    "  df_results_reordered.to_csv(output_path, index=False)\n",
    "  print(f\"\\nAll results saved to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 954
    },
    "id": "w9JBvKNQjgWy",
    "outputId": "25733f4d-ab2f-45fa-97d2-01a552aa309e"
   },
   "outputs": [],
   "source": [
    "# @title main function to run in Semi Batch Mode\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset_folder = 'D:/VM Data/thesis/Analysis/visium/'\n",
    "    output_csv = os.path.join(dataset_folder, \"visium_magic_knn_soft_simple_imputation_results.csv\")\n",
    "    n_top = ['all', 2000, 5000]\n",
    "    # n_top = ['all']\n",
    "\n",
    "    batch_size = 1  # Change this to control how many datasets to process per run\n",
    "    # n_top_batch_size = 1\n",
    "\n",
    "    # Get all .h5ad dataset filenames\n",
    "    all_datasets = sorted([f for f in os.listdir(dataset_folder) if f.endswith('.h5ad')])\n",
    "\n",
    "    # Get names of already processed datasets from the CSV\n",
    "    if os.path.exists(output_csv):\n",
    "        df_done = pd.read_csv(output_csv)\n",
    "        processed_datasets = set(df_done['Dataset Name'].unique())\n",
    "        print(processed_datasets)\n",
    "    else:\n",
    "        processed_datasets = set()\n",
    "\n",
    "    # Find remaining datasets to process\n",
    "    remaining_datasets = [f for f in all_datasets if f not in processed_datasets]\n",
    "    print(remaining_datasets)\n",
    "\n",
    "    if not remaining_datasets:\n",
    "        print(\"✅ All datasets are already processed and saved in the CSV.\\n\")\n",
    "    else:\n",
    "        print(f\"🟡 Found {len(remaining_datasets)} remaining datasets.\\n\")\n",
    "        to_process = remaining_datasets[:batch_size]\n",
    "\n",
    "        for dataset_filename in tqdm(to_process, desc=\"Processing Datasets in Bath-mode\"):\n",
    "            dataset_path = os.path.join(dataset_folder, dataset_filename)\n",
    "            all_results = []\n",
    "\n",
    "            for n in n_top:\n",
    "              print(f\"Analyzing dataset {dataset_filename} for top genes {n}\\n\")\n",
    "              df_row = run_on_single_dataset(dataset_path, n_top_genes=n)\n",
    "              all_results.append(df_row)\n",
    "            print(f\"✔ Analyzing finished for {dataset_filename}\\n\")\n",
    "            print(f\"save the results for the dataset {dataset_filename} in the csv\\n\")\n",
    "\n",
    "            # Merge all DataFrames\n",
    "            final_df = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "            # Sort by Dataset name, then by Top_Genes value\n",
    "            final_df = final_df.sort_values(by=[\"Dataset Name\", \"top_genes\"]).reset_index(drop=True)\n",
    "\n",
    "            # reorder the results\n",
    "            df_results_reordered = reorder_columns(final_df)\n",
    "\n",
    "            # Append to CSV\n",
    "            if os.path.exists(output_csv):\n",
    "                df_results_reordered.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "            else:\n",
    "                df_results_reordered.to_csv(output_csv, index=False)\n",
    "\n",
    "            print(f\"Results saved for the dataset {dataset_filename} in the csv!!\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fIvyJu0eAuNn",
    "outputId": "c6bf66ef-00a9-43ba-e3b6-6add7b82c732"
   },
   "outputs": [],
   "source": [
    "# @title main function to run in Multi-batch Mode\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset_folder = 'D:/VM Data/thesis/Analysis/visium/'\n",
    "    output_csv = os.path.join(dataset_folder, \"visium_scVI_GIMVI_tangram_imputation_results.csv\")\n",
    "\n",
    "    all_n_top = ['all', 2000, 5000]\n",
    "    batch_size = 1            # Number of datasets per batch\n",
    "    n_top_batch_size = 1     # Number of top_genes values per dataset per batch\n",
    "\n",
    "    all_datasets = sorted([f for f in os.listdir(dataset_folder) if f.endswith('.h5ad')])\n",
    "\n",
    "    # Load already processed entries\n",
    "    processed_map = {}\n",
    "    if os.path.exists(output_csv):\n",
    "        df_done = pd.read_csv(output_csv)\n",
    "        for dataset in df_done['Dataset Name'].unique():\n",
    "            processed_tops = set(df_done[df_done['Dataset Name'] == dataset]['top_genes'].astype(str))\n",
    "            processed_map[dataset] = processed_tops\n",
    "    else:\n",
    "        df_done = pd.DataFrame()\n",
    "        processed_map = {}\n",
    "\n",
    "    # Build dictionary of work to be done\n",
    "    work_dict = {}\n",
    "    for fname in all_datasets:\n",
    "        dataset_name = fname\n",
    "        done_tops = processed_map.get(dataset_name, set())\n",
    "        remaining_tops = [str(t) for t in all_n_top if str(t) not in done_tops]\n",
    "        if remaining_tops:\n",
    "            work_dict[dataset_name] = remaining_tops\n",
    "\n",
    "    # Print all pending work\n",
    "    if not work_dict:\n",
    "        print(\"✅ All datasets and top_genes combinations are already processed.\\n\")\n",
    "    else:\n",
    "        print(f\"🟡 {len(work_dict)} datasets still need processing:\")\n",
    "        for k, v in work_dict.items():\n",
    "            print(f\"  🔸 {k}: Missing top_genes → {v}\")\n",
    "        print()\n",
    "\n",
    "    # Select datasets to process\n",
    "    selected_datasets = list(work_dict.keys())[:batch_size]\n",
    "\n",
    "    for dataset_filename in tqdm(selected_datasets, desc=\"Processing Datasets in Batch-mode\"):\n",
    "        dataset_path = os.path.join(dataset_folder, dataset_filename)\n",
    "        remaining_tops = work_dict[dataset_filename]\n",
    "\n",
    "        # Handle smaller-than-batch case safely\n",
    "        top_genes_list = remaining_tops[:n_top_batch_size] if len(remaining_tops) >= n_top_batch_size else remaining_tops\n",
    "\n",
    "        all_results = []\n",
    "        for n in top_genes_list:\n",
    "            print(f\"🔍 Analyzing dataset {dataset_filename} for top genes {n}\")\n",
    "            df_row = run_on_single_dataset(dataset_path, n_top_genes=n)\n",
    "            all_results.append(df_row)\n",
    "\n",
    "        if all_results:\n",
    "            final_df = pd.concat(all_results, ignore_index=True)\n",
    "            final_df = final_df.sort_values(by=[\"Dataset Name\", \"top_genes\"]).reset_index(drop=True)\n",
    "            df_results_reordered = reorder_columns(final_df)\n",
    "\n",
    "            if os.path.exists(output_csv):\n",
    "                df_results_reordered.to_csv(output_csv, mode='a', header=False, index=False)\n",
    "\n",
    "                # Reload entire CSV and sort\n",
    "                full_df = pd.read_csv(output_csv)\n",
    "                full_df = full_df.sort_values(by=[\"Dataset Name\", \"top_genes\"]).reset_index(drop=True)\n",
    "                full_df = reorder_columns(full_df)\n",
    "                full_df.to_csv(output_csv, index=False)\n",
    "\n",
    "            else:\n",
    "                df_results_reordered.to_csv(output_csv, index=False)\n",
    "\n",
    "            print(f\"✅ Results saved for {dataset_filename}\\n\")\n",
    "\n",
    "    # 🔁 Print remaining work again after processing\n",
    "    print(\"🔄 Remaining datasets and top_genes still to process:\")\n",
    "    updated_processed = set()\n",
    "    if os.path.exists(output_csv):\n",
    "        updated_df = pd.read_csv(output_csv)\n",
    "        updated_processed_map = {\n",
    "            dataset: set(updated_df[updated_df['Dataset Name'] == dataset]['top_genes'].astype(str))\n",
    "            for dataset in updated_df['Dataset Name'].unique()\n",
    "        }\n",
    "        for fname in all_datasets:\n",
    "            done_tops = updated_processed_map.get(fname, set())\n",
    "            still_pending = [str(t) for t in all_n_top if str(t) not in done_tops]\n",
    "            if still_pending:\n",
    "                print(f\"  🔸 {fname}: Missing top_genes → {still_pending}\")\n",
    "    else:\n",
    "        print(\"⚠ CSV not found after run, no updates made.\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
